{
 "metadata": {
  "name": "",
  "signature": "sha256:66364abfa7a3a2e9871e0d0b2cb08163fad8b2cf5a2d159bc82290be6b10ed14"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "GCAL Tutorial"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This tutorial shows how to use the Topographica software package to\n",
      "explore an orientation map simulation using test patterns and weight\n",
      "plots.\n",
      "\n",
      "The simulations explored in this notebook use the GCAL model\n",
      " [(Stevens et al., J. Neuroscience 2013)](http://www.jneurosci.org/content/33/40/15747.full)\n",
      ", a relatively simple and easy-to-understand cortical model. As\n",
      "described in Bednar \n",
      "[(J. Physiology-Paris, 106:194-211 2013)](http://dx.doi.org/10.1016/j.jphysparis.2011.12.001)\n",
      ", much more complex and realistic simulations can be built using the\n",
      "same underlying components in Topographica, e.g. by including multiple\n",
      "cell classes and laminae in each simulated region, or simulating finer\n",
      "time steps, but this model is already capable of surprisingly complex\n",
      "behavior.\n",
      "\n",
      "This tutorial is also compatible with the earlier LISSOM model, which\n",
      "has been used extensively in publications (e.g. \n",
      "[Miikkulainen et al., 2005](http://computationalmaps.org/)). We have\n",
      "made it easy to select the LISSOM model at the start of the notebook\n",
      "and it should be possible to follow the rest of the tutorial without\n",
      "any additional changes, though of course any mention of GCAL\n",
      "explicitly should be updated to say LISSOM instead.\n",
      "\n",
      "An [advanced tutorial](http://ioam.github.io/media/topo/GCAL_Collector.html)\n",
      "is also available, demonstrating a more sophisticated analysis of the\n",
      "GCAL model as used in the published paper. This second GCAL tutorial\n",
      "is more suited to users already familiar with Topographica."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Using the IPython Notebook"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you can double-click on this text and edit it, you are in a live\n",
      "[IPython Notebook](http://ipython.org/notebook) environment where you\n",
      "can run the code and explore the model. Otherwise, you are viewing a\n",
      "static (e.g. HTML) copy of the notebook, which allows you to see the\n",
      "precomputed results only. To switch to the live notebook, see the\n",
      "[notebook installation\n",
      "instructions](https://github.com/ioam/topographica).\n",
      "\n",
      "We will start the notebook by importing the various components we will\n",
      "be using. You can run the following code cell by pressing\n",
      "``Shift+Enter`` or clicking the 'play' button from the menu above:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%reload_ext topo.misc.ipython"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os, math\n",
      "import numpy as np\n",
      "\n",
      "import topo\n",
      "import param\n",
      "import imagen\n",
      "\n",
      "import dataviews\n",
      "from imagen import analysis\n",
      "from dataviews import operation\n",
      "\n",
      "from featuremapper.command import measure_or_pref\n",
      "\n",
      "from topo.command import load_snapshot, save_snapshot, runscript\n",
      "from topo.command.analysis import measure_response, measure_cog\n",
      "\n",
      "from topo.analysis import Collector\n",
      "from topo.misc.ipython import RunProgress"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you find the plots in the notebook are either too big or too small, you may choose a more appropriate display size for your screen (default 100%):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%view webm:5 100 "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Loading the model snapshot"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Although this tutorial has been designed to run the GCAL model by\n",
      "default, it is compatible with the older LISSOM model. You can switch\n",
      "to the LISSOM model by setting the model variable to ``'LISSOM'`` in\n",
      "cell below:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model = 'GCAL'  # The model can either be 'GCAL' or 'LISSOM'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We require model to have trained for 10000 iterations with a small set of measurements taken every 1000 steps. To avoid repeating this procedure each time the notebook is executed, a snapshot of the model after 10000 is loaded from file. If an appropriate snapshot file cannot be found, one will be created to speed up subsequent runs of the notebook:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center>\n",
      "<h3 class='alert alert-info'>While the command is running, the notification ``Kernel busy`` will be displayed in the top right corner of the notebook.</h3>\n",
      "<center/>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "basename = 'gcal' if model=='GCAL' else 'lissom_oo_or'\n",
      "snapshot_name = basename + \"_10000.typ\"\n",
      "\n",
      "try:\n",
      "    snapshot_path = param.resolve_path(snapshot_name)\n",
      "    rebuild_snapshot = False\n",
      "except:\n",
      "    rebuild_snapshot = True"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "times = [i*1000 for i in range(11)]                        # Run 10000 iterations\n",
      "\n",
      "if rebuild_snapshot:\n",
      "    print \"Cannot locate %r. The required snapshot will now be regenerated...\" % snapshot_name\n",
      "    c = Collector()\n",
      "    # OR preference measurement\n",
      "    c.collect(measure_or_pref)                             # Measure the orientation preference\n",
      "    c.collect(measure_cog)                                 # Measure the Center of Gravity\n",
      "    \n",
      "    # Steps for rebuilding and saving the appropriate snapshot\n",
      "    model_dir = param.resolve_path('examples' if model=='GCAL' else 'models', path_to_file=False)\n",
      "    runscript(os.path.join(model_dir, basename + \".ty\"))  # Load the model    \n",
      "    topo.sim.views = c(times=times)\n",
      "    save_snapshot(snapshot_name)\n",
      "else:\n",
      "    load_snapshot(snapshot_name)\n",
      "    print \"Snapshot file %r loaded...\" % snapshot_name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Model architecture"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now the model is loaded, we shall first examine the training stimuli, the model architecture and the response to a simple test stimulus."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "An example of the patterns used in training"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To check the model has been loaded correctly, we can have a look at an example of the patterns that were presented on the retina sheet during training:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "imagen.Animation(pattern=topo.sim.Retina.input_generator, frames=50)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This small orientation map simulation should load in a few seconds,\n",
      "with a 78x78 retina, a 60x60 LGN (composed of one 60x60 OFF channel\n",
      "sheet, and one 60x60 ON channel sheet), and a 48x48 V1 with about two\n",
      "million synaptic weights.\n",
      "\n",
      "The model architecture is shown below:\n",
      "\n",
      "<center>\n",
      "<img src=\"http://topographica.org/_images/gcal_network_diagram.png\" \\>\n",
      "<center/>\n",
      "\n",
      "We now define the four sheet names for later reference:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sheets = ['Retina','LGNOn','LGNOff','V1']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Presenting a test stimulus"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will now define a test pattern to present to the network:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_pattern = imagen.Line()\n",
      "test_pattern[:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This shows the Line pattern with unit bounds (-0.5 to 0.5 in sheet\n",
      "coordinates) with a high sampling density used for display in the\n",
      "notebook. Here is the response to this particular test stimuli after\n",
      "this pattern is installed on the retinal sheet and presented to the\n",
      "network:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "response = measure_response(inputs=['Retina'], outputs=sheets, pattern_generator=test_pattern)\n",
      "response.LineResponse.grid(ordering=sheets)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This shows the response for each neural area. For these plots, you can\n",
      "see the [sheet\n",
      "coordinates](http://topographica.org/User_Manual/space.html#sheet-coordinates)\n",
      "along the x and y axes whereas the position of the array values is\n",
      "expressed in [matrix\n",
      "coordinates](http://topographica.org/User_Manual/space.html#matrix-coordinates). Note\n",
      "that the retinal sheet of the model has a larger bounds and a lower\n",
      "density than used for pattern display above, which explains why the\n",
      "line appears thinner and more coarsely sampled.\n",
      "\n",
      "In the Retina plot, each photoreceptor is represented as a pixel whose\n",
      "shade of gray codes the response level. Similarly, locations in the\n",
      "LGN that have an OFF or ON cell response to this pattern are shown in\n",
      "the LGNOff and LGNOn plots. At this stage the response level in V1 is\n",
      "also coded in shades of gray.\n",
      "\n",
      "From these plots, you can see that the single line presented on the\n",
      "retina is edge-detected in the LGN, with ON LGN cells responding to\n",
      "areas brighter than their surround, and OFF LGN cells responding to\n",
      "areas darker than their surround. In V1, the response is patchy, as\n",
      "explained below."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Activity statistics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The plots above show the spatial pattern of activity in the different\n",
      "sheets but do not show the absolute activity values. The best way to\n",
      "determine the absolute activity levels is to first get a handle on the\n",
      "raw activity array:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "activity_array = response.LineResponse.V1.last.data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now it is easy to examine the various statistics of the activity array:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "activity_info = activity_array.min(), activity_array.max(), activity_array.mean(), activity_array.std()\n",
      "print \"V1 Activity statistics -  min: %.3f, max %.3f, mean %.3f, std %.3f\" % activity_info"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The weights to a V1 neuron"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To help understand the response patterns in V1, we can look at the\n",
      "weights to V1 neurons. These weights were learned previously, as a\n",
      "result of presenting 10000 pairs of oriented Gaussian patterns at\n",
      "random angles and positions. Here are the synaptic strengths of\n",
      "connections to the neuron close to the center of the cortex for all\n",
      "the different incoming projections:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "coord = (0,0.1) # Close to the center of V1 \n",
      "(topo.sim.V1.LGNOffAfferent.view(*coord) + topo.sim.V1.LGNOnAfferent.view(*coord)\n",
      " + topo.sim.V1.LateralExcitatory.view(*coord) + topo.sim.V1.LateralInhibitory.view(*coord))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The plot shows the afferent weights to V1 (i.e., connections from the\n",
      "ON and OFF channels of the LGN), followed by the lateral excitatory\n",
      "and lateral inhibitory weights to that neuron from nearby neurons in\n",
      "V1. The afferent weights represent the retinal pattern that would most\n",
      "excite the neuron. For the particular neuron shown above, the optimal\n",
      "retinal stimulus would be a short, bright line oriented at about 35\n",
      "degrees (from 7 o\u2019clock to 1 o\u2019clock) in the center of the\n",
      "retina. (Note that the particular neuron you are viewing may have a\n",
      "different preferred orientation.)\n",
      "\n",
      "Note that the weights are shown on different spatial scales."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center>\n",
      "<h3 class='alert alert-info'>Try changing the coordinate of the V1 neuron. You can re-run a single cell repeatedly using ``Ctrl+Enter``</h3>\n",
      "</center>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If all neurons had the same weight pattern, the response would not be\n",
      "patchy \u2013 it would just be a blurred version of the input (for inputs\n",
      "matching the weight pattern), or blank (for other inputs). Here are\n",
      "what the afferent connections to the other neurons look like:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "topo.sim.V1.LGNOnAfferent.grid() + topo.sim.V1.LGNOffAfferent.grid()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This plot shows the afferent weights from the LGN ON sheet for 20\n",
      "neurons sampled in each direction. You can see that most of the\n",
      "neurons are selective for orientation (not just a circular spot), and\n",
      "each has a slightly different preferred orientation. This suggests an\n",
      "explanation for why the response is patchy: neurons preferring\n",
      "orientations other than the one present on the retina do not\n",
      "respond. You can also look at the LateralInhibitory weights instead of\n",
      "LGNOnAfferent; those are patchy as well because the typical activity\n",
      "patterns are patchy."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The V1 Orientation map"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To visualize all the neurons at once in experimental animals, optical\n",
      "imaging experiments measure responses to a variety of patterns and\n",
      "record the one most effective at stimulating each neuron. This\n",
      "approach of recording the cortical response to particular stimuli was\n",
      "used to measurements the maps in the model every 1000 iterations as it\n",
      "was trained to a total of 20000 iterations. This data is now made\n",
      "easily accessible as follows:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = topo.sim.views              # The measurement data saved in the snapshot"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here is a view of the orientation preference for neurons in V1 and the\n",
      "corresponding map of orientation selectivity:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pref_sel = (data.OrientationPreference.V1 * data.OrientationSelectivity.V1)\n",
      "(data.OrientationPreference.V1 + data.OrientationSelectivity.V1 + pref_sel)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center>\n",
      "<img src=\"http://topographica.org/_images/or_key_horiz_transparent.png\" width=\"50%\"/>\n",
      "<center/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Orientation Preference plot is the orientation map for V1 in this\n",
      "model. Each neuron in the plot is color coded by its preferred\n",
      "orientation, according to the key shown to the left of the plot.\n",
      "\n",
      "\n",
      "You can see that nearby neurons have similar orientation preferences,\n",
      "as found in primate visual cortex. The orientation selectivity plot\n",
      "shows the relative selectivity of each neuron for orientation on an\n",
      "arbitrary scale; you can see that in this simulation nearly all\n",
      "neurons became orientation selective. The middle plot (the polar plot)\n",
      "combines orientation preference and selectivity \u2013 each neuron is\n",
      "colored with its preferred orientation, and the stronger the\n",
      "selectivity, the brighter the color. In this case, because the neurons\n",
      "are strongly selective, the polar preference and selectivity plot is\n",
      "nearly identical to the preference plot."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Combined Activity and Orientation Preference"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "colorized_response = operation.HCS(data.OrientationPreference.V1.last * response.LineResponse.V1, flipSC=True)\n",
      "response.LineResponse.Retina + response.LineResponse.LGNOff + response.LineResponse.LGNOn + colorized_response"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center>\n",
      "<img src=\"http://topographica.org/_images/or_key_horiz_transparent.png\" width=\"50%\"/>\n",
      "<center/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Each V1 neuron is now color coded by its orientation, with brighter\n",
      "colors indicating stronger activation.\n",
      "\n",
      "The color coding allows us to see that the neurons responding are\n",
      "indeed those that prefer orientations similar to the input pattern,\n",
      "and that the response is patchy because other nearby neurons do not\n",
      "respond. To be sure of that, try using a line with a different\n",
      "orientation as a test stimulus \u2013 the colors should be different, and\n",
      "should match the orientation chosen."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "V1 Weights and their orientation preferences"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%opts CFView cmap='gray'\n",
      "coord = (0,0.1) # Close to the center of V1\n",
      "V1_exc = operation.Colorize(topo.sim.V1.LateralExcitatory.view(*coord, situated=True) \n",
      "                   * data.OrientationPreference.V1.last)\n",
      "V1_inh = operation.Colorize(topo.sim.V1.LateralInhibitory.view(*coord, situated=True) \n",
      "                            * data.OrientationPreference.V1.last)\n",
      "(topo.sim.V1.LGNOffAfferent.view(*coord, situated=True) \n",
      " + topo.sim.V1.LGNOnAfferent.view(*coord, situated=True) + V1_exc + V1_inh)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center>\n",
      "<img src=\"http://topographica.org/_images/or_key_horiz_transparent.png\" width=\"50%\"/>\n",
      "<center/>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Look at the LateralExcitatory weights, which show that the neurons near the above neuron are nearly all yellow-green, to match its preferred orientation.\n",
      "\n",
      "Note that the weights are now shown in grayscale so the only color corresponds to the orientation color key.\n",
      "\n",
      "Now let's have a look at the activity using a vertical line as a test stimulus:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_pattern = imagen.Line(orientation=math.pi/2)\n",
      "response = measure_response(inputs=['Retina'], outputs=sheets, pattern_generator=test_pattern)\n",
      "colorized_response = operation.HCS(data.OrientationPreference.V1.last * response.LineResponse.V1, flipSC=True)\n",
      "response.LineResponse.Retina + response.LineResponse.LGNOff + response.LineResponse.LGNOn + colorized_response"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%opts CFView cmap='gray'\n",
      "coord = (0,0.2) # A coordinate of a unit in the green path\n",
      "V1_exc = operation.Colorize(topo.sim.V1.LateralExcitatory.view(*coord, situated=True) \n",
      "                   * data.OrientationPreference.V1.last)\n",
      "V1_inh = operation.Colorize(topo.sim.V1.LateralInhibitory.view(*coord, situated=True) \n",
      "                            * data.OrientationPreference.V1.last)\n",
      "(topo.sim.V1.LGNOffAfferent.view(*coord, situated=True) \n",
      " + topo.sim.V1.LGNOnAfferent.view(*coord, situated=True) + V1_exc + V1_inh)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "The Fourier power spectrum"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As another example, an interesting property of orientation maps measured in animals is that their Fourier power spectrums usually show a ring shape, because the orientations repeat at a constant spatial frequency in all directions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fft = imagen.analysis.fft_power_spectrum(data.OrientationPreference.V1)\n",
      "fft.last"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<center>\n",
      "<h3 class='alert alert-info'>Try removing ``.last`` to see how the FFT evolves over development!\n",
      "</center>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Changing the test stimulus"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have already seen one example of how to change the test stimulus when we presented a vertical line to the network. Note that ``imagen.Line`` is only one of many different types of pattern that can be presented to the network. Here are some commonly used test stimuli:\n",
      "\n",
      "* ``imagen.Gaussian``\n",
      "* ``imagen.Disk``\n",
      "* ``imagen.Gabor``\n",
      "* ``imagen.SineGrating``\n",
      "* ``imagen.image.FileImage``\n",
      "* ``imagen.Arc``\n",
      "* ``imagen.Ring``\n",
      "* ``imagen.Rectangle``\n",
      "\n",
      "For a more complete list of available patterns, type ``imagen.`` into a code cell and press ``TAB``."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Common pattern parameters: **\n",
      "\n",
      "<dl class=\"dl-horizontal\" style=\"font-size:medium;\">\n",
      "<dt>x and y</dt> <dd> Controls the position on the retina (try 0 or 0.5) </dd>\n",
      "<dt>orientation</dt> <dd> Controls the angle (try $\\frac{\\pi}{4}$ or $\\frac{-\\pi}{4}$) </dd>\n",
      "<dt>size</dt> <dd> Controls the overall size of e.g. Gaussians and rings </dd>\n",
      "<dt>aspect_ratio</dt> <dd> Controls the ratio between width and height; will be scaled by the size in both directions</dd>\n",
      "<dt>smoothing</dt> <dd> Controls the amount of Gaussian falloff around the edges of patterns such as rings and lines </dd>\n",
      "<dt>scale</dt> <dd> Controls the brightness (try 1.0 for a sine grating). </dd>\n",
      "<dt>offset</dt> <dd> A value to be added to every pixel </dd>\n",
      "<dt>frequency</dt> <dd> Controls frequency of a sine grating or Gabor </dd>\n",
      "<dt>phase</dt> <dd> Controls phase of a sine grating or Gabor </dd>\n",
      "<dt>mask_shape</dt> <dd> Allows the pattern to be masked by another pattern (e.g. try a mask_shape of Disk with a SineGrating).</dd>\n",
      "</dl>\n",
      "\n",
      "***Note: how this model is insensitive to the scale; the response remains orientation selective even as the scale is varied substantially. (If you try the [lissom_oo_or](http://topographica.org/Tutorials/lissom_oo_or.html) tutorial, you can see the effect of contrast gain control operating in this model.)***\n",
      "\n",
      "\n",
      "As we have seen, a vertical line can be obtained by setting the ``orientation`` parameter for a ``Line`` to $\\frac{\\pi}{2}$ radians. Here is a different example, showing a SineGrating oriented at a 45$^o$ angle:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "imagen.SineGrating(orientation=math.pi/4)[:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can also view more about the different possible parameters of a test pattern using the %params magic as follows:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%params test_pattern"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the live notebook environment, this will open up a pane with details about all the available parameters for the pattern.\n",
      "\n",
      "<center>\n",
      "<h3 class='alert alert-info'>Try changing the stimulus and exploring different parameters.</h3>\n",
      "</center>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Presenting images from file"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To present photographs, select a Pattern generator of type\n",
      "``imagen.image.FileImage``. For most photographs, you will probably\n",
      "want to enlarge the image size to look at details. Here is an example\n",
      "of how a file image can be loaded and displayed:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filename = param.resolve_path('images/ellen_arthur.pgm')\n",
      "scale = 2.0 if model == 'LISSOM' else 1.0\n",
      "test_pattern = imagen.image.FileImage(filename=filename, scale=scale)\n",
      "test_pattern[:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that the scale needs to be boosted to make the network respond in\n",
      "the LISSOM model whereas the scale doesn't need to be adjusted for the\n",
      "GCAL model. This is due to the contrast gain-control mechanism that\n",
      "operates in the LGN sheets of the GCAL model.\n",
      "\n",
      "A much larger, more complicated, and slower map would be required to\n",
      "see interesting patterns in the response to most images, but even with\n",
      "this network you may be able to see some orientation-specific\n",
      "responses to large contours in the image:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "response = measure_response(inputs=['Retina'], outputs=sheets, pattern_generator=test_pattern)\n",
      "colorized_response = operation.HCS(data.OrientationPreference.V1.last * response.FileImageResponse.V1, flipSC=True)\n",
      "(response.FileImageResponse.Retina + response.FileImageResponse.LGNOff \n",
      " + response.FileImageResponse.LGNOn + colorized_response)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that the axes have very different limits between the different\n",
      "sheets. The ``Retina`` sheet extends past 1.5 units in [sheet\n",
      "Coordinates](http://topographica.org/User_Manual/space.html#sheet-coordinates)\n",
      "whereas the ``V1`` sheet doesn't quite reach 0.5 units in sheet\n",
      "coordinates. The reason for these differences is that in this\n",
      "particular network, the Retina and LGN stages each have an extra\n",
      "\u201cbuffer\u201d region around the outside so that no V1 neuron will have its\n",
      "CF cut off, and the result is that V1 sees only the central region of\n",
      "the image in the LGN, and the LGN sees only the central region of the\n",
      "retina."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Animation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here is an animation showing the preference, selectivity and combined preference/selectivity plots over development (top row). The bottom row show the FFT of the preference map and the Center of Gravity plots for the LGN ON and OFF projections."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(data.OrientationPreference.V1 + data.OrientationSelectivity.V1 + pref_sel\n",
      "+ fft + data.CoG.LGNOnAfferent + data.CoG.LGNOffAfferent).cols(3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}